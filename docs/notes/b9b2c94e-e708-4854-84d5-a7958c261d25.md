---
id: b9b2c94e-e708-4854-84d5-a7958c261d25
title: Upernet
desc: ''
updated: 1601113885454
created: 1601113885454
data: {}
fname: topic.segment.upernet
stub: false
parent: 06861511-cce1-4f66-9da7-b953161e61d2
children: []
hpath: topic.segment.upernet
---
# Upernet

## Meta Data:
- #citekey: _xiaoUnifiedPerceptualParsing2018_
- [[Xiao, Tete]], [[Liu, Yingcheng]], [[Zhou, Bolei]], [[Jiang, Yuning]], & [[Sun, Jian]] #authors
- [[2018]] #issued
- #title: Unified Perceptual Parsing for Scene Understanding
- #topic: [[Segmentation]], [[PSPNet]], [[FPN]]
## [[mmsegmentation config]]
  - model: upernet_r50
  - default_runtime
  - schedule: 80k, 160k
## Motivation: 
  - Since scene recognition, object detection, texture and material recognition are intertwined in human visual perception, this raises an important question for the computer vision systems: is it possible for a neural network to solve several visual recognition tasks simultaneously? This motives our work to introduce a new task called [[Unified Perceptual Parsing (UPP)]] along with a novel learning method to address it.
## Challenges:
  - 1. There is no single image dataset annotated with all levels of visual information. Various image datasets are constructed only for a specific task, such as [[ADE20K]]for scene parsing, the [[Describe Texture Dataset]] (DTD) for texture recognition, and OpenSurfaces for material and surface recognition.
  - 2. Annotations from different perceptual levels are heterogeneous. For example, [[ADE20K]] has pixel-wise annotations while the annotations for textures in the DTD are image-level.
## Solution:
  - 1. At each iteration, we randomly sample a data source, and <mark>only update the related layers</mark> on the path to infer the concepts from the selected source. Such a design avoids erratic behavior that the gradient with respect to annotations of a certain concept may be noisy. 
  - 2. Our framework exploits the hierarchical nature of features from a single network, i.e., for concepts with higher-level semantics such as scene classification, the classifier is built on the feature map with the higher semantics only; for lower-level semantics such as object and material segmentation, classifiers are built on feature maps fused across all stages or the feature map with low-level semantics only.
## 1. Unified Perceptual Parsing (UPP)
  - Recognition of many visual concepts as possible from a given image.
  - Possible visual concepts are: scene labels, objects and object parts; materials and textures of objects.
  - Construct an image dataset by combining several sources of image annotations.
      - Multi-source dataset enforce balancing
    ### Metrics 
    - **Pixel Accuracy (P.A.)**: proportion of correctly classified pixels
    - **IoU (mIoU)**: intersection-over-union between predicted and ground truth pixels, averaged over all object classes.
    - To balance samples across different labels in different categories we first randomly sample 10% of original images as the validation set. 
    - Then randomly choose an image both from the training and validation set, and check if the annotations in pixel-level are more balanced towards 10% after swapping these two images. The process is performed iteratively.
## 2. UPP Network
  - UPerNet:: Unified Perceptual Parsing Network, based on [[FPN]]. Apply a [[Pyramid Pooling Module (PPM)]] from [[PSPNet]] on the last layer of the backbone network before feeding to the top-down branch in [[FPN]]. <mark>The aim is to unify parsing of visual attributes at multiple levels.</mark>
 
  ((ref: [[fpn]]#fpn,1))
  - [[FCN]]: Fully convolutional networks. To enable high-resolution predictions, [[Dilated Net]] is adopted.
  - [[Dilated Net]]:
  ((ref: [[dilated net]]#dilated net,1))
  - 
    ### Network architecture
    - [[ResNet 101]] $ \{C_2,C_3,C_4,C_5\} $ and [[FPN]] $ \{P_2,P_3,P_4,P_5\} $ where $ P_5 $ follows [[Pyramid Pooling Module (PPM)]].  
    - **Scene label**: the highest level attribute annotated at image-level.
        - Predicted by a [[Global Average Pooling]] of $ P_5 $ followed by a linear classifier.  
        - Unlike [[Dilated Net]], the down-sampling rate of $ P_5 $ is large so the features after [[Global Average Pooling]] focus more on high-level semantics.
    - **Object label**: fusing all feature maps of [[FPN]] is better than only using the highest resolution ($ P_2 $).
    - Materials: On top of $P_2$ rather than fused features.
    - **Texture label**: given at image-level, is based on non-natural images.
    - ![https://remnote-user-data.s3.amazonaws.com/tTVklhTOhsJ_HMBdrVcdaw8KCn59cMDzRySYaUIYZpWwMfyvNrhPbMzrLo8uWg-Sbir72RiPq0bQ-ISHhJihozh2wImbuMkTeIt9Jp3_izdE1ILmElkJGww9PCodVcXB](https://remnote-user-data.s3.amazonaws.com/tTVklhTOhsJ_HMBdrVcdaw8KCn59cMDzRySYaUIYZpWwMfyvNrhPbMzrLo8uWg-Sbir72RiPq0bQ-ISHhJihozh2wImbuMkTeIt9Jp3_izdE1ILmElkJGww9PCodVcXB)
    - UPerNet framework for Unified Perceptual Parsing.
        - **Top-left**: The Feature Pyramid Network (FPN) with a PPM appended on the last layer of the back-bone network before feeding to the top-down branch in FPN.
        - **Top-right**: We use features at various semantic levels. Scene head is attached on the feature map directly after the PPM since image-level information is more suitable for scene classification.
        - **Bottom**: The illustrations of different heads.
